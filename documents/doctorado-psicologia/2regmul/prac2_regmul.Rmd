---
title: "Practica 2: Regresión múltiple, control estadístico y parcialización"
subtitle: "Doctorado psicología - FACSO Universidad de Chile"
author: "Juan Carlos Castillo"
output:
  html_document:
    theme: flatly
    highlight: tango
    toc: true
    toc_float: true
    toc_depth: 2
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introducción

En esta práctica nos enfocaremos en la comprensión del control estadístico a partir de la visualización de datos y la regresión parcial

# Ejemplo 1: ingreso, educación y experiencia.

## Librerías y datos

```{r}
pacman::p_load(dplyr,
               corrplot,
               ggplot2,
               texreg,
               stargazer)

datos=read.csv("https://juancarloscastillo.github.io/metsoc-facsouchile/documents/presentaciones/6regmul2/ingedexp.csv", sep="")
```

Ver datos

```{r}
stargazer(datos, type = "text", digits=0)
```

## Correlaciones y scatters

```{r}
cormat=datos %>% select(ingreso,educacion,experiencia) %>% cor()

round(cormat, digits=2)
corrplot.mixed(cormat)
```


```{r}
ggplot(datos, aes(x=educacion, y=ingreso)) + geom_point() +
  geom_smooth(method=lm)

ggplot(datos, aes(x=experiencia, y=ingreso)) + geom_point() +
  geom_smooth(method=lm, se=FALSE)

ggplot(datos, aes(x=educacion, y=experiencia)) + geom_point() +
  geom_smooth(method=lm, se=FALSE)
```

## Regresiones
```{r}
reg_y_x1=lm(ingreso ~ educacion, data=datos)
reg_y_x2=lm(ingreso ~ experiencia, data=datos)
reg_y_x1_x2=lm(ingreso ~ educacion + experiencia , data=datos)



screenreg(list(reg_y_x1,reg_y_x2,reg_y_x1_x2),digits = 1)
```


```{r results='asis', message=FALSE}
htmlreg(list(reg_y_x1,reg_y_x2,reg_y_x1_x2))
```




## Parciales

```{r}

reg_x1_x2=lm(educacion ~ experiencia , data=datos)

x1_fit_x2=fitted.values(reg_x1_x2)
resx1_2=residuals(reg_x1_x2)

datos=cbind(datos, x1_fit_x2,resx1_2)
datos

regy_resx1_2=lm(datos$ingreso ~ resx1_2)


screenreg(list(reg_y_x1,reg_y_x2,reg_y_x1_x2,regy_resx1_2), booktabs = TRUE, dcolumn = TRUE, doctype = FALSE, caption=" ")

```

¿Qué nos indica el coeficiente de regresión parcial?
¿Cómo se podría obtener el segundo coeficiente?


# Predictores categóricos

Hasta ahora hemos trabajado solamente con predictores a los que asumimos un nivel de medición contínua (es decir, al menos intervalar). ¿Qué sucede con predictores donde se asume un distinto nivel de medición, como nominal u ordinal? En general este tipo de predictores requiere una interpretación y tratamiento distinto que los predictores continuos.

## Predictores dicotómicos

Las variables dicotómicas son aquellas variables nominales u ordinales que poseen solo dos categorías de respuesta, por ejemplo hombre/mujer, sano/enfermo, deportista/sedentario. La inclusión de estas variables en un modelo de regresión no requiere un tratamiento especial, solo hay que considerar que su interpretación tiene un sentido distinto.

### Ejemplo


```{r}

# librerias:

pacman::p_load(dplyr,
               ggplot2,
               stargazer,
               kableExtra,
               descr,
               expss)

#datos

exercise <- read.csv("https://juancarloscastillo.github.io/metsoc-facsouchile/documents/practicas/3regmul/exercise.txt", sep="")

exercise

# transformando la variable peso x 100 para que sea más fácil de interpretar:

exercise$wtloss <- exercise$wtloss*100

```

Veamos primero un scatter de peso $Y$ con sexo como independiente $X_{sexo}$


```{r}
ggplot(exercise, aes(x=sex, y=wtloss)) + geom_point() +
  geom_smooth(method=lm, se=FALSE)
```

Observamos que solamente hay 2 niveles en los valores de la variable X: 0 (hombre) y 1 (mujer). Por lo tanto, solo dos medias condicionales. Vamos  a comparar con los promedios simples por sexo, pero antes vamos a etiquetar los valores de la variable sexo mediante las funciones de la librería `sjlabelled`

Si calculamos el promedio simple de perdida de peso por sexo tenemos:


```{r}
val_lab(exercise$sex) = num_lab("
             0 Hombre
             1 Mujer
")

exercise$sex=as.factor(exercise$sex)

```


```{r}
exercise %>%
    group_by(sex) %>%
summarize(Mean = mean(wtloss))

```


Segun esto la pérdida de peso para una mujer es de 825 gramos, mientras para un hombre es de 700. Realizando ahora la regresión:


```{r}
reg1<-lm(wtloss ~ sex, data=exercise)
stargazer(reg1, type = "text")

```


Entonces,

$$wtloss=700 + 125Sexo$$

Las mujeres pierden en promedio 125 gramos más en comparación con los hombres. En este caso, _hombre_ se denomina la _categoría de referencia_

¿Cuál es la predicción de pérdida de peso para la variable sexo?

En el caso de hombre:

$$wtloss=700 + 125*0=700$$
Y para mujer:

$$wtloss=700 + 125*1=825$$

... que finalmente es lo mismo que los valores promedio de pérdida de peso para hombre y mujer que vimos antes al calcular los promedios simples. Por lo tanto:

- al ingresar un regresor dicotómico en regresión simple lo que se obtiene es una estimación de la diferencia de promedios de ambas categorías en relación a la variable dependiente
-en regresión múltiple esta diferencia se ajusta o controla por la presencia de otras variables, por ejemplo:

```{r}
reg2 <-  lm(wtloss ~ sex + food, data=exercise)

```
```{r}
stargazer(list(reg1,reg2), type = "text")
```

Lo que sucede aquí es que controlando por ingesta de alimentos (food), las mujeres pierden en promedio 136 más gramos que los hombres. ¿Cómo es posible que un control haga aumentar una diferencia? Veamos los promedios de ingesta de alimentos según sexo:

```{r}
exercise %>%
    group_by(sex) %>%
summarize(Mean = mean(food))

```

Los hombres en promedio ingieren más alimentos que las mujeres, y a pesar de eso sabemos que en general pierden menos peso. Por lo tanto, controlando por ingesta de alimentos hace aumentar la diferencia.


## Predictores con más de una categoría

LA base `exercise` contiene una variable del mismo nombre que contiee información sobr las horas de ejercicio:


```{r}
freq(exercise$exercise)
```


Las horas de ejercicio pueden ser tomadas como una variable continua o categórica, lo que depende no solo de la distribución empírica de la variable sino principalmente de la decisión de quien investiga. Para este ejercicio la tomaremos como una variable categórica de tres niveles: bajo (0), medio (2) y alto (4).

Para poder incluir esta variable en la regresión como categórica en R la manera más simple es definirla como un factor. Primero necesitamos conocer la estructura de la variable, ya que puede venir previamente definida como factor:

```{r}
str(exercise$exercise)
```
En este caso, `int` hace referencia a una variable contínua. Si la ingresamos de esta manera a la regresión:

```{r}
reg3 <- lm(wtloss ~ exercise, data=exercise)
reg3
```

Este $b$ nos dice que por cada nivel adicional de ejercicio, la perdida de peso promedio es de 175 gramos. Para transformarla a factor la función es `as.factor`, lo que puede ser definido en la misma regresión:

```{r}
reg4 <- lm(wtloss ~ as.factor(exercise), data=exercise)
reg4

```

La diferencia es que aquí aparecen 2 betas. El primero `as.factor(exercise)2` que se refiere a 2 horas y `as.factor(exercise)4` a 4 horas. Al ser ingresado como predictor categórico, al igual que en los dicotómicos del ejemplo anterior la interpretación se realiza en relación a la categoría de referencia. En este caso, sabemos que la variable tiene tres niveles de respuesta (0,2,4) y que al ser estimada como factor automáticamente `R` deja como nivel de referencia al más bajo (0). Y, por lo tanto, los betas de un predictor estimado como categórico por defecto serán equivalentes a los niveles de la variable -1. En este caso, la interpretación es que el ejercitar 2 horas hace que en promedio la perdida de peso sea de 350 gramos más que no ejercitar nada (referencia), y para el caso de ejercitar 4 horas en promedio lleva a una pérdida de peso de 700 gramos en relación a no ejercitar nada.

Para entender esto, veamos los promedios de perdida de peso por la variable ejercicio:


```{r}
exercise %>%
    group_by(exercise) %>%
summarize(Mean = mean(wtloss))

```


```{r}
ggplot(exercise, aes(x=exercise, y=wtloss)) + geom_point() +
  geom_smooth(method=lm, se=FALSE)
```



Con esta información analicemos con más detalle qué sucede con la estimación en la regresión. De acuerdo al modelo con `exercise` estimado como categórico:

$$wtloss=400 + exercise_{2horas}350+exercise_{4horas}700$$

Es decir, para una persona que ejercita 2 horas el $\widehat{Y}$ es 400+350=750, para quien ejercita 4 es 400+700=1100, y para quien ejercita 0 es solo 400, lo que es equivalente a los promedios simples calculados anteriormente. Y si dividimos la diferencia del nivel más bajo (400) y el más alto (1100) por los niveles teóricos de esta variable (4 horas) tenemos: 700/4=175, que sería la pérdida de peso por cada hora adicional de ejercicio, y que corresponde al coeficiente del predictor estimado como continuo.

Un par de cosas más:


- Para que las etiquetas hagan luego más sentido en el reporte:

```{r}
val_lab(exercise$exercise) = num_lab("
             0 Bajo
             2 Medio
             4 Alto
")

# Y transformar previamente a factor

exercise$exercise<-as.factor(exercise$exercise)


reg4 <- lm(wtloss ~ exercise, data=exercise)
reg4


```

Y/o establecer mejores etiquetas en reporte final con `stargazer`


```{r}
stargazer(reg4,
          type = "text",
          covariate.labels=c("Ejercicio Medio", "Ejercicio Alto"),
          dep.var.caption  = "Pérdida de peso"
)
```


- es posible cambiar la categoría de referencia. Por ejemplo, si quisieramos que la referencia fuera 2 horas:

```{r}
reg5 <- lm(formula = wtloss ~ relevel(exercise, ref="Medio"), data = exercise)

reg5
```



# Inferencia


## Ejemplo cálculo de prueba t

Cálculo de la desviación estándar de la diferencia de promedios:



```{r}
exercise %>%
    group_by(sex) %>%
summarize(var = var(wtloss))

exercise %>%
    group_by(sex) %>%
count()

# Desviación estándar de la diferencia
sigma_diff=((152000*5)+(62500*3))/(6+4-2)

sigma_diff
```


Error estándar de la diferencia

```{r}
sqrt((sigma_diff/6)+(sigma_diff/4))

```



```{r}
exercise %>%
    group_by(sex) %>%
summarize(Mean = mean(wtloss))

t=125/222.146
t

```


Comparando
```{r}
stargazer(reg1, type="text")

```


Intervalos de confianza

```{r}
qt(p = c(0.025, 0.975), df = 8)

# limite inferior
1.250+(-2.306*2.221)

# Limite superior
1.250+(2.306*2.221)


confint(reg1,level=0.95)




```
